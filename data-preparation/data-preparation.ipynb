{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0753029-788f-47e4-b9a7-7cb1f3c9e2b8",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "\n",
    "I leveraged Claude to put together some code to extract data from iNaturalist (https://www.inaturalist.org) for common Arizona plant families and genera\n",
    "\n",
    "The ```iNaturalistSpeciesScraper``` is a utility class to talk to the iNaturalist API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f000d6-5ad5-48d7-b2cf-eea60ef9e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "class iNaturalistSpeciesScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://api.inaturalist.org/v1\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Plant RAG Dataset Builder'\n",
    "        })\n",
    "    \n",
    "    def search_arizona_desert_plants(self):\n",
    "        \"\"\"Search for common Arizona desert plant families and genera\"\"\"\n",
    "        \n",
    "        # Common Arizona desert plant groups\n",
    "        search_terms = [\n",
    "            # Cacti\n",
    "            'Carnegiea', 'Ferocactus', 'Echinocactus', 'Mammillaria', \n",
    "            'Opuntia', 'Cylindropuntia',\n",
    "            # Agaves and relatives\n",
    "            'Agave', 'Yucca', 'Dasylirion', 'Nolina',\n",
    "            # Desert trees and shrubs\n",
    "            'Parkinsonia', 'Prosopis', 'Olneya', 'Cercidium',\n",
    "            'Fouquieria', 'Larrea', 'Acacia', 'Simmondsia',\n",
    "            # Wildflowers and perennials\n",
    "            'Penstemon', 'Lupinus', 'Eschscholzia', 'Baileya',\n",
    "            'Encelia', 'Sphaeralcea', 'Calliandra',\n",
    "            # Grasses and groundcovers\n",
    "            'Muhlenbergia', 'Bouteloua', 'Hesperaloe',\n",
    "        ]\n",
    "        \n",
    "        all_taxa_ids = set()\n",
    "        \n",
    "        for term in search_terms:\n",
    "            print(f\"Searching for: {term}\")\n",
    "            params = {\n",
    "                'q': term,\n",
    "                'place_id': 35,  # Arizona\n",
    "                'is_active': True,\n",
    "                'per_page': 50\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = self.session.get(f\"{self.base_url}/taxa/autocomplete\", params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                results = data.get('results', [])\n",
    "                for taxon in results:\n",
    "                    # Only include species and subspecies\n",
    "                    if taxon.get('rank') in ['species', 'subspecies', 'variety']:\n",
    "                        all_taxa_ids.add(taxon['id'])\n",
    "                \n",
    "                print(f\"  Found {len(results)} taxa\")\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error searching for {term}: {e}\")\n",
    "        \n",
    "        print(f\"\\nTotal unique taxa found: {len(all_taxa_ids)}\")\n",
    "        return list(all_taxa_ids)\n",
    "    \n",
    "    def get_taxon_details(self, taxon_id):\n",
    "        \"\"\"Get detailed information about a specific taxon\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/taxa/{taxon_id}\")\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('results'):\n",
    "                return data['results'][0]\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting taxon {taxon_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_rich_plant_data(self, taxon):\n",
    "        \"\"\"Extract comprehensive plant information from taxon data\"\"\"\n",
    "        if not taxon:\n",
    "            return None\n",
    "        \n",
    "        plant_data = {\n",
    "            'taxon_id': taxon.get('id'),\n",
    "            'scientific_name': taxon.get('name'),\n",
    "            'common_name': taxon.get('preferred_common_name'),\n",
    "            'rank': taxon.get('rank'),\n",
    "            \n",
    "            # Rich text content\n",
    "            'wikipedia_summary': taxon.get('wikipedia_summary', ''),\n",
    "            'wikipedia_url': taxon.get('wikipedia_url', ''),\n",
    "            \n",
    "            # Taxonomy\n",
    "            'kingdom': None,\n",
    "            'phylum': None,\n",
    "            'class': None,\n",
    "            'order': None,\n",
    "            'family': None,\n",
    "            'genus': None,\n",
    "            \n",
    "            # Conservation and status\n",
    "            'conservation_status': None,\n",
    "            'conservation_status_source': None,\n",
    "            'threatened': taxon.get('threatened', False),\n",
    "            'endemic': taxon.get('endemic', False),\n",
    "            'native': taxon.get('native', False),\n",
    "            'introduced': taxon.get('introduced', False),\n",
    "            \n",
    "            # Observations\n",
    "            'observations_count': taxon.get('observations_count', 0),\n",
    "            \n",
    "            # Images\n",
    "            'default_photo_url': taxon.get('default_photo', {}).get('medium_url') if taxon.get('default_photo') else None,\n",
    "            'photo_urls': [photo.get('medium_url') for photo in taxon.get('taxon_photos', [])[:5]],\n",
    "            \n",
    "            # Additional info\n",
    "            'iconic_taxon_name': taxon.get('iconic_taxon_name'),\n",
    "        }\n",
    "        \n",
    "        # Extract taxonomic hierarchy\n",
    "        if taxon.get('ancestors'):\n",
    "            for ancestor in taxon['ancestors']:\n",
    "                rank = ancestor.get('rank')\n",
    "                if rank in ['kingdom', 'phylum', 'class', 'order', 'family', 'genus']:\n",
    "                    plant_data[rank] = ancestor.get('name')\n",
    "        \n",
    "        # Conservation status\n",
    "        if taxon.get('conservation_status'):\n",
    "            status = taxon['conservation_status']\n",
    "            plant_data['conservation_status'] = status.get('status')\n",
    "            plant_data['conservation_status_source'] = status.get('authority')\n",
    "        \n",
    "        return plant_data\n",
    "    \n",
    "    def fetch_all_species_details(self, taxa_ids, max_species=None):\n",
    "        \"\"\"Fetch detailed information for all taxa\"\"\"\n",
    "        all_plants = []\n",
    "        total = len(taxa_ids)\n",
    "        \n",
    "        if max_species:\n",
    "            taxa_ids = taxa_ids[:max_species]\n",
    "            total = max_species\n",
    "        \n",
    "        print(f\"\\nFetching details for {total} species...\")\n",
    "        \n",
    "        for i, taxon_id in enumerate(taxa_ids, 1):\n",
    "            print(f\"Progress: {i}/{total} - Taxon ID: {taxon_id}\")\n",
    "            \n",
    "            taxon = self.get_taxon_details(taxon_id)\n",
    "            if taxon:\n",
    "                plant_data = self.extract_rich_plant_data(taxon)\n",
    "                if plant_data:\n",
    "                    # Only include if it has some descriptive content\n",
    "                    if plant_data['wikipedia_summary']:\n",
    "                        all_plants.append(plant_data)\n",
    "                        print(f\"  ✓ {plant_data['scientific_name']} - {len(plant_data['wikipedia_summary'])} chars\")\n",
    "                    else:\n",
    "                        print(f\"  ✗ {plant_data.get('scientific_name', 'Unknown')} - No description\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return all_plants\n",
    "    \n",
    "    def save_to_csv(self, plants_data, filename=\"arizona_desert_plants.csv\"):\n",
    "        \"\"\"Save plant data to CSV\"\"\"\n",
    "        df = pd.DataFrame(plants_data)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\n✓ Saved {len(plants_data)} plant records to {filename}\")\n",
    "    \n",
    "    def save_to_json(self, plants_data, filename=\"arizona_desert_plants.json\"):\n",
    "        \"\"\"Save plant data to JSON\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(plants_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✓ Saved {len(plants_data)} plant records to {filename}\")\n",
    "    \n",
    "    def generate_summary_stats(self, plants_data):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DATASET SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nTotal species: {len(plants_data)}\")\n",
    "        \n",
    "        # Species with descriptions\n",
    "        with_desc = sum(1 for p in plants_data if p.get('wikipedia_summary'))\n",
    "        print(f\"Species with descriptions: {with_desc}\")\n",
    "        \n",
    "        # Average description length\n",
    "        desc_lengths = [len(p.get('wikipedia_summary', '')) for p in plants_data if p.get('wikipedia_summary')]\n",
    "        if desc_lengths:\n",
    "            avg_length = sum(desc_lengths) / len(desc_lengths)\n",
    "            print(f\"Average description length: {avg_length:.0f} characters\")\n",
    "        \n",
    "        # Family distribution\n",
    "        families = defaultdict(int)\n",
    "        for p in plants_data:\n",
    "            if p.get('family'):\n",
    "                families[p['family']] += 1\n",
    "        \n",
    "        print(f\"\\nTop 10 families:\")\n",
    "        for family, count in sorted(families.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  {family}: {count}\")\n",
    "        \n",
    "        # Conservation status\n",
    "        conserved = sum(1 for p in plants_data if p.get('conservation_status'))\n",
    "        print(f\"\\nSpecies with conservation status: {conserved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa1add-1ad8-4957-bad4-63ba717c3a6c",
   "metadata": {},
   "source": [
    "## Fetching the data\n",
    "\n",
    "The code below using the scraper utility class to obtain ~637 plant species/families from Arizona\n",
    "\n",
    "Two files are generated: arizona_desert_plants.csv and arizona_desert_plants.json and saved locally\n",
    "\n",
    "**Note**: this takes ~10 minutes to run; there are rate limits in the API and the code takes a conservative approach of one request per species/family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd844f5f-0fde-440f-9bb2-45d1bf4c7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "scraper = iNaturalistSpeciesScraper()\n",
    "\n",
    "print(\"Step 1: Searching for Arizona desert plant species...\")\n",
    "taxa_ids = scraper.search_arizona_desert_plants()\n",
    "\n",
    "print(f\"\\nStep 2: Fetching detailed information for {len(taxa_ids)} species...\")\n",
    "print(\"(This will take a while - about 1 second per species)\")\n",
    "\n",
    "plants_data = scraper.fetch_all_species_details(taxa_ids)\n",
    "\n",
    "print(f\"\\nStep 3: Saving data...\")\n",
    "scraper.save_to_csv(plants_data)\n",
    "scraper.save_to_json(plants_data)\n",
    "\n",
    "scraper.generate_summary_stats(plants_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Done! Check the generated CSV and JSON files.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00cd64-5c19-45ee-ba5b-3fbd661b87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plants_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60602b68-b7af-4b87-9227-6b1814b9a77f",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "Let's examine the data we have obtained so far and see whether we can use it as-is or it needs some cleanup. We will read the plants_data.csv file since we have it handy already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cf31b8-638c-4d6b-9b83-807042fbfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfba48-57d5-45c0-b409-84d5ebe64ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = pd.read_json('arizona_desert_plants.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c7329-366c-46dd-8d98-db74c03be652",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28c26b-6dca-4408-b525-d23aa0458f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de2b28-a4b8-441f-a4e9-37d9937ceac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the ones that don't have a common name\n",
    "dp[dp['common_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3f607-57a2-4ff5-9d22-6ab36f7af3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove species w/o a common name\n",
    "dp_filtered = dp[dp['common_name'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572811a4-d26c-45db-a5a3-739654040650",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7a98a-d6b2-40e5-be6b-0363f4f6730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "dp_filtered.to_json('arizona_desert_plants_clean.json', \n",
    "                    orient='records',  # makes sure json objects are written as records\n",
    "                    indent=2)  # Pretty print for readability :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e852f73",
   "metadata": {},
   "source": [
    "# Additional data\n",
    "While the iNaturalist data is a good start, the descriptions are not enough to be able to provide sound knowledge and answers for the intent of the project. \n",
    "\n",
    "Again, leveraging Claude, to extract data, in PDF format, from the University of Arizona Extensions department.\n",
    "\n",
    "The ```UAExtensionsScrapper``` is a utility class to download PDFs that contain information about desert plant care, plant types, landscaping and design, etc.\n",
    "\n",
    "**Note:** The final number of dowloaded PDFs is 14. I originally started with a list that only yield 6 PDFs, so I then tried another set after which I obtained the final 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d170843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAExtensionScraper:\n",
    "    def __init__(self, download_dir=\"extension_pdfs\"):\n",
    "        self.download_dir = Path(download_dir)\n",
    "        self.download_dir.mkdir(exist_ok=True)\n",
    "        self.text_dir = Path(\"extension_texts\")\n",
    "        self.text_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'UA Extension PDF Scraper'\n",
    "        })\n",
    "        \n",
    "        # Curated list of Arizona Extension publications about desert plants\n",
    "        # URLs verified as working in October 2024\n",
    "        # I put together a bigger list based on experimentation\n",
    "        self.publication_urls = [\n",
    "            # Core desert plant care (Working)\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1048.pdf\",  # Care of Desert Adapted Plants\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1022.pdf\",  # Planting & Establishing Native & Adapted Trees\n",
    "            \n",
    "            # Specific plant types (Working)\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1048-2018.pdf\",  # Desert Adapted Plants\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1673.pdf\",  # Saguaro Cactus\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1429.pdf\",  # Palm Trees\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1722.pdf\",  # Agaves\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1021.pdf\",  # Arizona Landscape Palms\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az2021-2023.pdf\",  # Arizona Landscape Palms Management\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/attachment/Ocotillo.pdf\",  # Ocotillo Care\n",
    "            \n",
    "            # Landscaping & Design (Working)\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1455.pdf\",  # Desert Landscaping\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1624.pdf\",  # Xeriscaping\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1110.pdf\",  # Ground Covers\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1100a.pdf\",  # Flower Planting Guide\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1455.pdf\",  # Desert Landscaping\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1624.pdf\",  # Xeriscaping\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1298.pdf\",  # Watering\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1020.pdf\",  # Fertilizing\n",
    "            \n",
    "            # Container & Special Topics (Working)\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1713-2016.pdf\",  # Container Gardening\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1800-2019.pdf\",  # Prickly Pear Cactus\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/attachment/nopal_final_English.pdf\",  # Desert Foods - Nopal\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/attachment/Wildflowers.pdf\",  # Wildflowers & Native Grasses\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1713-2016.pdf\",  # Container Gardening\n",
    "\n",
    "            # Specific guides\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1456.pdf\",  # Native Plant Guide\n",
    "            \"https://extension.arizona.edu/sites/extension.arizona.edu/files/pubs/az1123.pdf\",  # Plant Selection\n",
    "\n",
    "            # Additional Resources\n",
    "            \"https://extension.arizona.edu/sites/default/files/2024-08/az1429.pdf\",  # Original Palm Trees (if different)\n",
    "        ]\n",
    "    \n",
    "    def download_pdf(self, url, custom_filename=None):\n",
    "        \"\"\"Download a single PDF\"\"\"\n",
    "        try:\n",
    "            print(f\"Downloading: {url}\")\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if custom_filename:\n",
    "                filename = custom_filename\n",
    "            else:\n",
    "                # Extract filename from URL\n",
    "                filename = os.path.basename(urlparse(url).path)\n",
    "                if not filename.endswith('.pdf'):\n",
    "                    filename += '.pdf'\n",
    "            \n",
    "            filepath = self.download_dir / filename\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f\"  ✓ Saved: {filename} ({len(response.content)/1024:.1f} KB)\")\n",
    "            return filepath\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  ✗ Error downloading {url}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Unexpected error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_text_pdfplumber(self, pdf_path):\n",
    "        \"\"\"Extract text using pdfplumber (more accurate)\"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {\n",
    "            'pages': 0,\n",
    "            'extraction_method': 'pdfplumber'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                metadata['pages'] = len(pdf.pages)\n",
    "                \n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += f\"\\n--- Page {page_num} ---\\n\"\n",
    "                        text += page_text + \"\\n\"\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ pdfplumber error on {pdf_path.name}: {e}\")\n",
    "            return None, metadata\n",
    "        \n",
    "        return text, metadata\n",
    "    \n",
    "    def extract_text_pypdf2(self, pdf_path):\n",
    "        \"\"\"Extract text using PyPDF2 (fallback)\"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {\n",
    "            'pages': 0,\n",
    "            'extraction_method': 'pypdf2'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                metadata['pages'] = len(pdf_reader.pages)\n",
    "                \n",
    "                for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += f\"\\n--- Page {page_num} ---\\n\"\n",
    "                        text += page_text + \"\\n\"\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ PyPDF2 error on {pdf_path.name}: {e}\")\n",
    "            return None, metadata\n",
    "        \n",
    "        return text, metadata\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean extracted text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        # Remove common header/footer patterns\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip page markers we added\n",
    "            if line.startswith('--- Page') and line.endswith('---'):\n",
    "                cleaned_lines.append(line)\n",
    "                continue\n",
    "            \n",
    "            # Skip very short lines that might be page numbers\n",
    "            if len(line) < 3:\n",
    "                continue\n",
    "            \n",
    "            # Skip lines that are just numbers (page numbers)\n",
    "            if re.match(r'^\\d+$', line):\n",
    "                continue\n",
    "            \n",
    "            # Skip common headers/footers\n",
    "            if any(pattern in line.lower() for pattern in [\n",
    "                'university of arizona',\n",
    "                'cooperative extension',\n",
    "                'college of agriculture',\n",
    "                'issued in furtherance'\n",
    "            ]):\n",
    "                continue\n",
    "            \n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and clean text from PDF\"\"\"\n",
    "        print(f\"Processing: {pdf_path.name}\")\n",
    "        \n",
    "        # Try pdfplumber first (better quality)\n",
    "        text, metadata = self.extract_text_pdfplumber(pdf_path)\n",
    "        \n",
    "        # Fallback to PyPDF2 if pdfplumber fails\n",
    "        if not text or len(text.strip()) < 100:\n",
    "            print(f\"  → Trying PyPDF2 fallback...\")\n",
    "            text, metadata = self.extract_text_pypdf2(pdf_path)\n",
    "        \n",
    "        if not text or len(text.strip()) < 100:\n",
    "            print(f\"  ✗ No meaningful text extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Clean the text\n",
    "        clean_text = self.clean_text(text)\n",
    "        \n",
    "        # Save to text file\n",
    "        txt_filename = pdf_path.stem + '.txt'\n",
    "        txt_path = self.text_dir / txt_filename\n",
    "        \n",
    "        with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(clean_text)\n",
    "        \n",
    "        print(f\"  ✓ Extracted {len(clean_text)} characters ({metadata['pages']} pages)\")\n",
    "        \n",
    "        return {\n",
    "            'pdf_filename': pdf_path.name,\n",
    "            'txt_filename': txt_filename,\n",
    "            'source_url': None,  # Will be set by caller\n",
    "            'text_length': len(clean_text),\n",
    "            'pages': metadata['pages'],\n",
    "            'extraction_method': metadata['extraction_method']\n",
    "        }\n",
    "    \n",
    "    def download_all_publications(self):\n",
    "        \"\"\"Download all curated publications\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"Downloading UA Extension Publications\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        downloaded = []\n",
    "        \n",
    "        for i, url in enumerate(self.publication_urls, 1):\n",
    "            print(f\"\\n[{i}/{len(self.publication_urls)}]\")\n",
    "            filepath = self.download_pdf(url)\n",
    "            if filepath:\n",
    "                downloaded.append((filepath, url))\n",
    "        \n",
    "        print(f\"\\n✓ Downloaded {len(downloaded)}/{len(self.publication_urls)} PDFs\")\n",
    "        return downloaded\n",
    "    \n",
    "    def process_all_pdfs(self):\n",
    "        \"\"\"Process all PDFs in download directory\"\"\"\n",
    "        pdf_files = list(self.download_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            print(\"No PDFs found in download directory!\")\n",
    "            return []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Processing {len(pdf_files)} PDFs\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, pdf_path in enumerate(pdf_files, 1):\n",
    "            print(f\"[{i}/{len(pdf_files)}]\")\n",
    "            result = self.process_pdf(pdf_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_metadata(self, results, filename=\"extension_publications_metadata.json\"):\n",
    "        \"\"\"Save processing results metadata\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\n✓ Saved metadata to {filename}\")\n",
    "    \n",
    "    def generate_summary(self, results):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXTRACTION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_pdfs = len(results)\n",
    "        total_pages = sum(r['pages'] for r in results)\n",
    "        total_chars = sum(r['text_length'] for r in results)\n",
    "        \n",
    "        print(f\"\\nSuccessfully processed: {total_pdfs} publications\")\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "        print(f\"Total text content: {total_chars:,} characters\")\n",
    "        print(f\"Average per document: {total_chars//total_pdfs:,} characters\")\n",
    "        \n",
    "        print(f\"\\nText files saved to: {self.text_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "scraper = UAExtensionScraper()\n",
    "\n",
    "print(\"Step 1: Downloading publications...\")\n",
    "downloaded = scraper.download_all_publications()\n",
    "\n",
    "if downloaded:\n",
    "    print(\"\\nStep 2: Extracting text from PDFs...\")\n",
    "    results = scraper.process_all_pdfs()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nStep 3: Saving metadata...\")\n",
    "        scraper.save_metadata(results)\n",
    "        \n",
    "        scraper.generate_summary(results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ Done! Check the 'extension_texts' folder for extracted content\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n✗ No text was successfully extracted\")\n",
    "else:\n",
    "    print(\"\\n✗ No PDFs were downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb4ba6",
   "metadata": {},
   "source": [
    "# Putting all the data together\n",
    "\n",
    "Now, we need to put it all together into a single dataset. Again, I enlisted Claude to help me write a script to combine the two data sets.\n",
    "\n",
    "The resulting dataset is stored in 3 different formats: csv, json and json lines (jsonl); the format of the filenames is arizona_plants_unified_YYYYMMDD.<csv|json|jsonl>\n",
    "\n",
    "**Note:** the code below uses simple chunking, for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261e37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class ArizonaPlantDatasetBuilder:\n",
    "    def __init__(self, \n",
    "                 inaturalist_json,\n",
    "                 extension_text_dir):\n",
    "        self.inaturalist_json = inaturalist_json\n",
    "        self.extension_text_dir = Path(extension_text_dir)\n",
    "        self.unified_dataset = []\n",
    "    \n",
    "    def load_inaturalist_data(self):\n",
    "        \"\"\"Load and process iNaturalist species data\"\"\"\n",
    "        print(\"Loading iNaturalist data...\")\n",
    "        \n",
    "        with open(self.inaturalist_json, 'r', encoding='utf-8') as f:\n",
    "            species_data = json.load(f)\n",
    "        \n",
    "        # Filter: only species with common names\n",
    "        species_data = [s for s in species_data if s.get('common_name')]\n",
    "        \n",
    "        print(f\"Loaded {len(species_data)} species with common names\")\n",
    "        \n",
    "        documents = []\n",
    "        for species in species_data:\n",
    "            # Create a rich text document for each species\n",
    "            doc_text = self._format_species_document(species)\n",
    "            \n",
    "            document = {\n",
    "                'id': f\"species_{species['taxon_id']}\",\n",
    "                'type': 'species',\n",
    "                'source': 'iNaturalist',\n",
    "                'title': f\"{species['common_name']} ({species['scientific_name']})\",\n",
    "                'content': doc_text,\n",
    "                'metadata': {\n",
    "                    'scientific_name': species['scientific_name'],\n",
    "                    'common_name': species['common_name'],\n",
    "                    'family': species.get('family'),\n",
    "                    'genus': species.get('genus'),\n",
    "                    'native': species.get('native'),\n",
    "                    'conservation_status': species.get('conservation_status'),\n",
    "                    'wikipedia_url': species.get('wikipedia_url'),\n",
    "                    'observations_count': species.get('observations_count', 0)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            documents.append(document)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _format_species_document(self, species):\n",
    "        \"\"\"Format species data into a readable document\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Title\n",
    "        lines.append(f\"# {species['common_name']}\")\n",
    "        lines.append(f\"*{species['scientific_name']}*\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Taxonomy\n",
    "        taxonomy = []\n",
    "        if species.get('family'):\n",
    "            taxonomy.append(f\"Family: {species['family']}\")\n",
    "        if species.get('genus'):\n",
    "            taxonomy.append(f\"Genus: {species['genus']}\")\n",
    "        if taxonomy:\n",
    "            lines.append(\"**Taxonomy:** \" + \", \".join(taxonomy))\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Description (Wikipedia summary)\n",
    "        if species.get('wikipedia_summary'):\n",
    "            lines.append(\"## Description\")\n",
    "            # Remove HTML tags from wikipedia summary\n",
    "            import re\n",
    "            clean_summary = re.sub(r'<[^>]+>', '', species['wikipedia_summary'])\n",
    "            lines.append(clean_summary)\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Status\n",
    "        status_info = []\n",
    "        if species.get('native'):\n",
    "            status_info.append(\"Native to Arizona\")\n",
    "        elif species.get('introduced'):\n",
    "            status_info.append(\"Introduced species\")\n",
    "        \n",
    "        if species.get('conservation_status'):\n",
    "            status_info.append(f\"Conservation Status: {species['conservation_status']}\")\n",
    "        \n",
    "        if status_info:\n",
    "            lines.append(\"**Status:** \" + \", \".join(status_info))\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Observations\n",
    "        if species.get('observations_count'):\n",
    "            lines.append(f\"*{species['observations_count']} observations recorded in Arizona*\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def load_extension_documents(self):\n",
    "        \"\"\"Load and process Extension publication texts\"\"\"\n",
    "        print(\"Loading Extension publications...\")\n",
    "        \n",
    "        text_files = list(self.extension_text_dir.glob(\"*.txt\"))\n",
    "        print(f\"Found {len(text_files)} text files\")\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for txt_file in text_files:\n",
    "            with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Extract title from filename (e.g., az1048.txt -> az1048)\n",
    "            doc_id = txt_file.stem\n",
    "            \n",
    "            # Try to extract title from first few lines\n",
    "            title = self._extract_title_from_content(content, doc_id)\n",
    "            \n",
    "            document = {\n",
    "                'id': f\"extension_{doc_id}\",\n",
    "                'type': 'extension_publication',\n",
    "                'source': 'University of Arizona Cooperative Extension',\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'metadata': {\n",
    "                    'publication_id': doc_id,\n",
    "                    'filename': txt_file.name,\n",
    "                    'content_length': len(content)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            documents.append(document)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _extract_title_from_content(self, content, default_id):\n",
    "        \"\"\"Try to extract title from document content\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        # Look for title in first 10 lines\n",
    "        for line in lines[:10]:\n",
    "            line = line.strip()\n",
    "            # Title is usually a longer line with capitals\n",
    "            if len(line) > 20 and len(line) < 100:\n",
    "                if any(word[0].isupper() for word in line.split() if word):\n",
    "                    return line\n",
    "        \n",
    "        return f\"Extension Publication {default_id}\"\n",
    "    \n",
    "    def chunk_long_documents(self, documents, max_chunk_size=2000):\n",
    "        \"\"\"Split long documents into smaller chunks for better retrieval\"\"\"\n",
    "        chunked_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            content = doc['content']\n",
    "            \n",
    "            # If document is short enough, keep as is\n",
    "            if len(content) <= max_chunk_size:\n",
    "                chunked_docs.append(doc)\n",
    "                continue\n",
    "            \n",
    "            # Split into chunks by paragraphs\n",
    "            paragraphs = content.split('\\n\\n')\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            chunk_num = 1\n",
    "            \n",
    "            for para in paragraphs:\n",
    "                para_length = len(para)\n",
    "                \n",
    "                # If adding this paragraph exceeds limit, save current chunk\n",
    "                if current_length + para_length > max_chunk_size and current_chunk:\n",
    "                    chunk_content = '\\n\\n'.join(current_chunk)\n",
    "                    \n",
    "                    chunk_doc = {\n",
    "                        'id': f\"{doc['id']}_chunk_{chunk_num}\",\n",
    "                        'type': doc['type'],\n",
    "                        'source': doc['source'],\n",
    "                        'title': f\"{doc['title']} (Part {chunk_num})\",\n",
    "                        'content': chunk_content,\n",
    "                        'metadata': {\n",
    "                            **doc['metadata'],\n",
    "                            'chunk_number': chunk_num,\n",
    "                            'is_chunk': True,\n",
    "                            'parent_id': doc['id']\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    chunked_docs.append(chunk_doc)\n",
    "                    \n",
    "                    # Start new chunk\n",
    "                    current_chunk = [para]\n",
    "                    current_length = para_length\n",
    "                    chunk_num += 1\n",
    "                else:\n",
    "                    current_chunk.append(para)\n",
    "                    current_length += para_length\n",
    "            \n",
    "            # Add final chunk\n",
    "            if current_chunk:\n",
    "                chunk_content = '\\n\\n'.join(current_chunk)\n",
    "                chunk_doc = {\n",
    "                    'id': f\"{doc['id']}_chunk_{chunk_num}\",\n",
    "                    'type': doc['type'],\n",
    "                    'source': doc['source'],\n",
    "                    'title': f\"{doc['title']} (Part {chunk_num})\" if chunk_num > 1 else doc['title'],\n",
    "                    'content': chunk_content,\n",
    "                    'metadata': {\n",
    "                        **doc['metadata'],\n",
    "                        'chunk_number': chunk_num,\n",
    "                        'is_chunk': chunk_num > 1,\n",
    "                        'parent_id': doc['id']\n",
    "                    }\n",
    "                }\n",
    "                chunked_docs.append(chunk_doc)\n",
    "        \n",
    "        return chunked_docs\n",
    "    \n",
    "    def build_unified_dataset(self, chunk_size=2000):\n",
    "        \"\"\"Build unified dataset from all sources\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"Building Unified Arizona Desert Plants Dataset\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Load both data sources\n",
    "        species_docs = self.load_inaturalist_data()\n",
    "        extension_docs = self.load_extension_documents()\n",
    "        \n",
    "        # Combine\n",
    "        all_documents = species_docs + extension_docs\n",
    "        print(f\"\\nTotal documents before chunking: {len(all_documents)}\")\n",
    "        print(f\"  - Species: {len(species_docs)}\")\n",
    "        print(f\"  - Extension publications: {len(extension_docs)}\")\n",
    "        \n",
    "        # Chunk long documents\n",
    "        print(f\"\\nChunking documents (max size: {chunk_size} chars)...\")\n",
    "        chunked_documents = self.chunk_long_documents(all_documents, chunk_size)\n",
    "        print(f\"Total documents after chunking: {len(chunked_documents)}\")\n",
    "        \n",
    "        return chunked_documents\n",
    "    \n",
    "    def save_dataset(self, documents, directory, output_format='json'):\n",
    "        \"\"\"Save unified dataset\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "        \n",
    "        if output_format == 'json':\n",
    "            filename = f\"{directory}/arizona_plants_unified_{timestamp}.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(documents, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\n✓ Saved JSON dataset: {filename}\")\n",
    "        \n",
    "        elif output_format == 'jsonl':\n",
    "            filename = f\"{directory}/arizona_plants_unified_{timestamp}.jsonl\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                for doc in documents:\n",
    "                    f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "            print(f\"\\n✓ Saved JSONL dataset: {filename}\")\n",
    "        \n",
    "        elif output_format == 'csv':\n",
    "            # Flatten for CSV\n",
    "            filename = f\"{directory}/arizona_plants_unified_{timestamp}.csv\"\n",
    "            df = pd.DataFrame([\n",
    "                {\n",
    "                    'id': doc['id'],\n",
    "                    'type': doc['type'],\n",
    "                    'source': doc['source'],\n",
    "                    'title': doc['title'],\n",
    "                    'content': doc['content'],\n",
    "                    'content_length': len(doc['content']),\n",
    "                    **{f\"meta_{k}\": v for k, v in doc['metadata'].items()}\n",
    "                }\n",
    "                for doc in documents\n",
    "            ])\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"\\n✓ Saved CSV dataset: {filename}\")\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def generate_dataset_report(self, documents):\n",
    "        \"\"\"Generate comprehensive dataset statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Overall stats\n",
    "        total_docs = len(documents)\n",
    "        total_chars = sum(len(doc['content']) for doc in documents)\n",
    "        avg_chars = total_chars // total_docs\n",
    "        \n",
    "        print(f\"\\nTotal Documents: {total_docs}\")\n",
    "        print(f\"Total Content: {total_chars:,} characters\")\n",
    "        print(f\"Average per Document: {avg_chars:,} characters\")\n",
    "        \n",
    "        # By type\n",
    "        by_type = {}\n",
    "        for doc in documents:\n",
    "            doc_type = doc['type']\n",
    "            by_type[doc_type] = by_type.get(doc_type, 0) + 1\n",
    "        \n",
    "        print(\"\\nDocument Types:\")\n",
    "        for doc_type, count in sorted(by_type.items()):\n",
    "            print(f\"  {doc_type}: {count}\")\n",
    "        \n",
    "        # Chunked vs original\n",
    "        chunked = sum(1 for doc in documents if doc['metadata'].get('is_chunk'))\n",
    "        print(f\"\\nChunked documents: {chunked}\")\n",
    "        print(f\"Original documents: {total_docs - chunked}\")\n",
    "        \n",
    "        # Sample content lengths\n",
    "        lengths = [len(doc['content']) for doc in documents]\n",
    "        lengths.sort()\n",
    "        print(f\"\\nContent length distribution:\")\n",
    "        print(f\"  Minimum: {lengths[0]:,} chars\")\n",
    "        print(f\"  Median: {lengths[len(lengths)//2]:,} chars\")\n",
    "        print(f\"  Maximum: {lengths[-1]:,} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf792eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = ArizonaPlantDatasetBuilder(\n",
    "    inaturalist_json=\"data-preparation/arizona_desert_plants_clean.json\",\n",
    "    extension_text_dir=\"extension_texts\"\n",
    ")\n",
    "\n",
    "# Build unified dataset with 2000 char chunks\n",
    "documents = builder.build_unified_dataset(chunk_size=2000)\n",
    "\n",
    "# Save in multiple formats\n",
    "dir = \"data-preparation\"\n",
    "print(\"\\nSaving dataset in multiple formats...\")\n",
    "builder.save_dataset(documents, dir, output_format='json')\n",
    "builder.save_dataset(documents, dir, output_format='jsonl')\n",
    "builder.save_dataset(documents, dir, output_format='csv')\n",
    "\n",
    "# Generate report\n",
    "builder.generate_dataset_report(documents)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Dataset integration complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pd.read_json('arizona_plants_unified_20251018.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
